{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d2ea45-0dcf-4943-b566-cac32a1c82e1",
   "metadata": {},
   "source": [
    "# 无条件图像生成扩散模型————用来生成美丽的蝴蝶图像"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cce829-e9d6-467b-8dab-2716fc7c9b4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 准备工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66d2ac-ee43-48c5-b37c-78c9e70ba5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-lsp-server[all]\n",
    "#!export HF_ENDPOINT='https://hf-mirror.com/'\n",
    "import os\n",
    "#更改huggingface网址\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com/'\n",
    "# ！echo 'export HF_ENDPOINT=\"https://hf-mirror.com\"' >> ~/.bashrc\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b8b27-ff4c-4ff0-9d6b-09e9abc6c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U diffusers datasets transformers accelerate ftfy pyarrow==9.0.0 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e36b45-96c7-416e-8ae5-62522e776d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def show_images(x):\n",
    "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
    "    x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)\n",
    "    grid = torchvision.utils.make_grid(x)\n",
    "    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n",
    "    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n",
    "    return grid_im\n",
    "\n",
    "\n",
    "def make_grid(images, size=64):\n",
    "    \"\"\"Given a list of PIL images, stack them together into a line for easy viewing\"\"\"\n",
    "    output_im = Image.new(\"RGB\", (size * len(images), size))\n",
    "    for i, im in enumerate(images):\n",
    "        output_im.paste(im.resize((size, size)), (i * size, 0))\n",
    "    return output_im\n",
    "\n",
    "\n",
    "# Mac users may need device = 'mps' (untested)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5cf1b1-09e1-4958-b51b-7779d2ff1ab6",
   "metadata": {},
   "source": [
    "## 核心内容\n",
    "- 可以直接使用DiffusionPipeline.from_pretrained获取pipeline，根据模型可输入num_inference_steps 采样步骤数量，guidance_scale 输出与提示的匹配程度生成图像\n",
    "- 也可自行构建model和scheduler组成pipeline，这里使用UNet2DModel 和 DDPMScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d290b21-6195-4bda-99db-05950014aeb1",
   "metadata": {},
   "source": [
    "### 数据集下载和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0670fb-7d8b-4c9a-bee3-c930e9c33b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "# Check out https://huggingface.co/sd-dreambooth-library for loads of models from the community\n",
    "# model_id = \"/root/lanyun-tmp/data\"\n",
    "model_id = \"/root/diffussion/my_pipeline/\"\n",
    "# model_id = \"sd-dreambooth-library/mr-potato-head\"\n",
    "\n",
    "# Load the pipeline \n",
    "# 只使用本地文件 local_files_only = True\n",
    "pipe = DiffusionPipeline.from_pretrained(model_id,local_files_only = True).to(\n",
    "    device\n",
    ")\n",
    "# pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16,local_files_only = True).to(\n",
    "#     device\n",
    "# )\n",
    "images = pipe(batch_size=8).images\n",
    "make_grid(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48d799c-90e3-477d-a01e-79ce72c9a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset ,Image\n",
    "\n",
    "# 本地数据集下载\n",
    "# !huggingface-cli download --resume-download --repo-type dataset huggan/smithsonian_butterflies_subset --local-dir /root/lanyun-tmp/data/smithsonian_butterflies_subset\n",
    "\n",
    "# 本地根据文件格式选用load_dataset方式\n",
    "# 更多本地文件载入方式：https://huggingface.co/docs/datasets/loading\n",
    "# 由于图像的传输一般会用到bytes或base64格式，这里是bytes格式，以二进制形式存储数据，需要使用from datasets import Image的Image方法进行解码，转换为PIL图片：.cast_column(\"image\", Image())，”image“是字典中图片的key\n",
    "# 更多载入图片数据方法：https://huggingface.co/docs/datasets/image_load\n",
    "base_url = \"/root/lanyun-tmp/data/smithsonian_butterflies_subset/data/\"\n",
    "data_files = {\"train\": base_url + \"train-00000-of-00001.parquet\"}\n",
    "dataset = load_dataset(\"parquet\", data_files=data_files, split=\"train\").cast_column(\"image\", Image())\n",
    "\n",
    "\n",
    "# We'll train on 32-pixel square images, but you can try larger sizes too\n",
    "image_size = 32\n",
    "# You can lower your batch size if you're running out of GPU memory\n",
    "batch_size = 64\n",
    "\n",
    "# Define data augmentations\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),  # Resize\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip (data augmentation)\n",
    "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "# 每次访问dataset都调用transform\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "# Create a dataloader from the dataset to serve up the transformed images in batches\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6000f246-482c-4226-ad53-8384d8aaa38d",
   "metadata": {},
   "source": [
    "### 查看数据集内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d4d55-38a3-4989-98c0-5956ab2f5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = next(iter(train_dataloader))[\"images\"].to(device)[:8]\n",
    "print(\"X shape:\", xb.shape)\n",
    "show_images(xb).resize((8 * 64, 64), resample=Image.Resampling.NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710ddab-93a6-468f-a1e3-0ad116079d05",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4394f50-9d7e-456c-907e-076ad8f70632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "# Create a model\n",
    "model = UNet2DModel(\n",
    "    sample_size=image_size,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(64, 128, 128, 256),  # More channels -> more parameters\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"AttnDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "    ),\n",
    ")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40420649-9d4c-4754-8c16-a2fb705714b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_prediction = model(noisy_xb, timesteps).sample\n",
    "model_prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b226c0-6488-4051-b3d0-49a739c4f87c",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d224e538-8fc7-43cb-bc18-c4555fc9a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# 设置噪声调度器\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=1000, beta_schedule=\"squaredcos_cap_v2\"\n",
    ")\n",
    "\n",
    "# 训练循环\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(30):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        clean_images = batch[\"images\"].to(device)\n",
    "        # 为图像添加噪声\n",
    "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "        bs = clean_images.shape[0]\n",
    "\n",
    "        # 为每个图像随机选择一个时间步长\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device\n",
    "        ).long()\n",
    "\n",
    "        # 根据每个时间步长的噪声幅度向干净图像添加噪声\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        # 获取模型的预测结果\n",
    "        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "        # 计算损失\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward(loss)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # 使用优化器更新模型参数\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        loss_last_epoch = sum(losses[-len(train_dataloader) :]) / len(train_dataloader)\n",
    "        print(f\"Epoch:{epoch+1}, loss: {loss_last_epoch}\")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training time: {training_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ed164-a2a4-4c4c-b7e4-573a22e015c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axs[0].plot(losses)\n",
    "axs[1].plot(np.log(losses))\n",
    "plt.show()\n",
    "np.min(losses)\n",
    "# 保存图片到文件\n",
    "fig.savefig('loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc13488c-2281-4371-b2f4-f0c3dd7e81cf",
   "metadata": {},
   "source": [
    "### 模型预测\n",
    "这里采用了两种方法，一种通过DDPMPipeline方法调用pipeline，另一种使用model和noise_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5bc19-0d52-48e8-b512-6d44ad599cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "image_pipe = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
    "pipeline_output = image_pipe()\n",
    "pipeline_output.images[0]\n",
    "#image_pipe.save_pretrained(\"my_pipeline\")\n",
    "\n",
    "# Random starting point (8 random images):\n",
    "sample = torch.randn(8, 3, 32, 32).to(device)\n",
    "\n",
    "for i, t in enumerate(noise_scheduler.timesteps):\n",
    "\n",
    "    # Get model pred\n",
    "    with torch.no_grad():\n",
    "        residual = model(sample, t).sample\n",
    "\n",
    "    # Update sample with step\n",
    "    sample = noise_scheduler.step(residual, t, sample).prev_sample\n",
    "\n",
    "show_images(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
